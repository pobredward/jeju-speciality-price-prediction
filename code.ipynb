{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98ae731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import calendar\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from itertools import permutations, combinations\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_contour, plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate, plot_slice, plot_param_importances\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from prophet import Prophet\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5193fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seed_tr = 3\n",
    "num_seed_hp = 1\n",
    "splits_hp = 5\n",
    "splits_tr = 10\n",
    "seed_hp = 42\n",
    "basic_seed = 42\n",
    "num_trial = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db25874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "international_trade_csv = pd.read_csv('data/international_trade.csv')\n",
    "submission_df = pd.read_csv('data/sample_submission.csv')\n",
    "# train_df = train_df.replace([np.inf, -np.inf, np.nan, -np.nan], 0)\n",
    "# test_df = test_df.replace([np.inf, -np.inf, np.nan, -np.nan], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e4b7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_train = {}\n",
    "for idx, time in enumerate(train_df['timestamp'].unique()):\n",
    "    time = time.replace('-', '')\n",
    "    dic_train[time] = idx+1\n",
    "    \n",
    "dic_test = {}\n",
    "for idx, time in enumerate(test_df['timestamp'].unique()):\n",
    "    time = time.replace('-', '')\n",
    "    dic_test[time] = idx+1524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074b5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시계열 특성을 학습에 반영하기 위해 timestamp를 월, 일, 시간으로 나눕니다\n",
    "train_df['year'] = train_df['timestamp'].apply(lambda x : int(x[0:4]))\n",
    "train_df['month'] = train_df['timestamp'].apply(lambda x : int(x[5:7]))\n",
    "train_df['day'] = train_df['timestamp'].apply(lambda x : int(x[8:10]))\n",
    "train_df['weekday'] = train_df['timestamp'].apply(lambda x : datetime.strptime(x, '%Y-%m-%d').weekday())\n",
    "train_df['prod_ID'] = train_df['ID'].apply(lambda x: x[0:6])\n",
    "train_df['d'] = train_df['ID'].apply(lambda x: f'd_{dic_train[x[7:]]}')\n",
    "# train_df.drop(['supply(kg)', 'timestamp'], axis=1, inplace=True)\n",
    "\n",
    "test_df['year'] = test_df['timestamp'].apply(lambda x : int(x[0:4]))\n",
    "test_df['month'] = test_df['timestamp'].apply(lambda x : int(x[5:7]))\n",
    "test_df['day'] = test_df['timestamp'].apply(lambda x : int(x[8:10]))\n",
    "test_df['weekday'] = test_df['timestamp'].apply(lambda x : datetime.strptime(x, '%Y-%m-%d').weekday())\n",
    "test_df['prod_ID'] = test_df['ID'].apply(lambda x: x[0:6])\n",
    "test_df['d'] = test_df['ID'].apply(lambda x: f'd_{dic_test[x[7:]]}')\n",
    "# test_df.drop(['timestamp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed28d13",
   "metadata": {},
   "source": [
    "이동 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2f2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.rename(columns={'price(원/kg)':'price', 'supply(kg)': 'supply'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db503802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_average(df):\n",
    "    for win in [1, 2, 4, 8]:\n",
    "        df['rm_diff_price_{}'.format(win)] = df[['item', 'corporation', 'location', 'price']].groupby(\n",
    "            ['item', 'corporation', 'location'])['price'].transform(lambda x : x.rolling(win).mean())\n",
    "        df['rm_diff_price_{}'.format(win)] = ((df['price'] - df['rm_diff_price_{}'.format(win)]\n",
    "                                                  )/df['price']).round(3)\n",
    "    return df\n",
    "train_df = get_moving_average(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8cde064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lags_wins(df):\n",
    "    lags = [7, 14, 28]\n",
    "    lag_cols = [f\"lag_{lag}\" for lag in lags]\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        df[lag_col] = df[[\"prod_ID\",\"price\"]].groupby(\"prod_ID\")[\"price\"].shift(lag)\n",
    "\n",
    "    wins = [7, 14, 28]\n",
    "    for win in wins :\n",
    "        for lag,lag_col in zip(lags, lag_cols):\n",
    "            df[f\"rmean_{lag}_{win}\"] = df[[\"prod_ID\", lag_col]].groupby(\"prod_ID\")[lag_col].transform(lambda x : x.rolling(win).mean())\n",
    "    return df\n",
    "train_df = lags_wins(train_df)\n",
    "train_df = train_df.replace([np.inf, -np.inf, np.nan, -np.nan], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19e0feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['price'] = 0\n",
    "test_df[['rm_diff_price_1', 'rm_diff_price_2', 'rm_diff_price_4',\n",
    "       'rm_diff_price_8', 'lag_7', 'lag_14', 'lag_28', 'rmean_7_7',\n",
    "       'rmean_14_7', 'rmean_28_7', 'rmean_7_14', 'rmean_14_14', 'rmean_28_14',\n",
    "       'rmean_7_28', 'rmean_14_28', 'rmean_28_28']] = 0.0\n",
    "test_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4473636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.astype({'ID': 'category', 'item': 'category', 'corporation': 'category', 'location': 'category',\n",
    "                            'year': 'category', 'month': 'category', 'day': 'category', 'weekday': 'category'})\n",
    "test_df = test_df.astype({'ID': 'category', 'item': 'category', 'corporation': 'category', 'location': 'category',\n",
    "                            'year': 'category', 'month': 'category', 'day': 'category', 'weekday': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03fd459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = train_df.copy()\n",
    "# for day in range(1496, 1524):\n",
    "#     valid_df.loc[valid_df['d']==f'd_{day}', ['price', 'rm_diff_price_1', 'rm_diff_price_2', 'rm_diff_price_4',\n",
    "#        'rm_diff_price_8', 'lag_7', 'lag_14', 'lag_28', 'rmean_7_7',\n",
    "#        'rmean_14_7', 'rmean_28_7', 'rmean_7_14', 'rmean_14_14', 'rmean_28_14',\n",
    "#        'rmean_7_28', 'rmean_14_28', 'rmean_28_28']] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90640c41",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7f9058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols = ['item','corporation', 'location', 'year', 'month', 'day', 'weekday']\n",
    "useless_cols = ['ID', 'prod_ID', 'd', 'supply', 'timestamp', 'price']\n",
    "train_cols = train_df.columns[~train_df.columns.isin(useless_cols)]\n",
    "df = train_df.copy()\n",
    "days_train = ['d_'+str(c) for c in range(1, 1496)]\n",
    "days_val = ['d_'+str(c) for c in range(1496, 1524)]\n",
    "df = df.replace([np.inf, -np.inf, np.nan, -np.nan], 0)\n",
    "#df.iloc[:, -9:] = df.iloc[:, -9:].fillna(0.0)\n",
    "X_train = df[df['d'].isin(days_train)==True][train_cols]\n",
    "Y_train = df[df['d'].isin(days_train)==True][\"price\"]\n",
    "X_val_df = df[df['d'].isin(days_val)==True]\n",
    "X_val_df[\"timestamp\"] = pd.to_datetime(X_val_df[\"timestamp\"])\n",
    "X_val_dff = pd.DataFrame()\n",
    "for delta in range(0, 28):\n",
    "    day = datetime(2023, 2, 4) + timedelta(days=delta)\n",
    "    vl = X_val_df.loc[X_val_df.timestamp == day]\n",
    "    X_val_dff = pd.concat([X_val_dff, vl])\n",
    "\n",
    "X_val = X_val_dff[train_cols]\n",
    "Y_val = X_val_dff[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fab09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(trial: Trial) -> float:\n",
    "    score_hp = []\n",
    "    for seed_hp in np.random.randint(0, 1000, num_seed_hp):\n",
    "        params_lgb = {\n",
    "            \"random_state\": seed_hp,\n",
    "            \"verbosity\": -1,\n",
    "            \"n_estimators\": 10000,\n",
    "            \"objective\": \"tweedie\",\n",
    "            \"metric\": \"rmse\",\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 5e-3, 5e-2), # default=0.1, range=[0,1]\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10), # default=-1\n",
    "            \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-2, 1e+2), # default=0\n",
    "            \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-2, 1e+2), # default=0\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 5000), # default=31, range=(1,130172]\n",
    "            \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.3, 1.0), # feature_fraction, default=1\n",
    "            \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 1.0), # bagging_fraction, default=1, range=[0,1]\n",
    "            \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 20), # bagging_freq, default=0\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 40), # min_data_in_leaf, default=20 \n",
    "#             \"max_bin\": trial.suggest_int(\"max_bin\", 100, 500),\n",
    "        }\n",
    "\n",
    "        kfold = KFold(n_splits=splits_hp, random_state=seed_hp, shuffle=True)\n",
    "        cv = np.zeros(X_train.shape[0])\n",
    "\n",
    "        for n, (train_idx, val_idx) in enumerate(kfold.split(X_train, Y_train)):\n",
    "\n",
    "            x_train, x_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_train, y_val = Y_train.iloc[train_idx].values, Y_train.iloc[val_idx].values\n",
    "\n",
    "            lgbmodel = LGBMRegressor(**params_lgb)\n",
    "                                                                                           \n",
    "            lgbmodel.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=30, verbose=-1) \n",
    "            cv[val_idx] = lgbmodel.predict(x_val)\n",
    "            \n",
    "        cv = np.where(cv<100, 0, cv)\n",
    "        score_hp.append(mean_squared_error(Y_train, cv)**0.5)\n",
    "        print(f'Seed{seed_hp} RMSE: {mean_squared_error(Y_train, cv)**0.5}')\n",
    "    \n",
    "    np.mean(score_hp)\n",
    "    return np.mean(score_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a65fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=basic_seed)\n",
    "lgb_study = optuna.create_study(study_name=\"lgb_parameter_opt\", direction=\"minimize\", sampler=sampler)\n",
    "lgb_study.optimize(lgb_objective, n_trials=num_trial)\n",
    "\n",
    "lgb_best_hyperparams = lgb_study.best_trial.params\n",
    "lgb_base_hyperparams = {'n_estimators':10000}\n",
    "lgb_best_hyperparams.update(lgb_base_hyperparams)\n",
    "\n",
    "# with open('../pkl/lgb_best_hyperparams.pickle', 'wb') as fw:\n",
    "#     pickle.dump(lgb_best_hyperparams, fw)\n",
    "print(\"The best hyperparameters are:\\n\", lgb_best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a255c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_best_hyperparams = {'metric': 'rmse', 'learning_rate': 0.01, \n",
    "                        'objective': 'rmse', 'boost_from_average': False,\n",
    "                        'n_estimators': 10000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d113efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_slice(lgb_study);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.matplotlib.plot_param_importances(lgb_study);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e67640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a43216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3900\n",
      "[LightGBM] [Info] Number of data points in the train set: 58305, number of used features: 22\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's rmse: 863.186\n",
      "[200]\tvalid_0's rmse: 543.209\n",
      "[300]\tvalid_0's rmse: 451.403\n",
      "[400]\tvalid_0's rmse: 420.445\n",
      "[500]\tvalid_0's rmse: 400.215\n",
      "[600]\tvalid_0's rmse: 393.525\n",
      "[700]\tvalid_0's rmse: 394.139\n",
      "[800]\tvalid_0's rmse: 392.468\n",
      "[900]\tvalid_0's rmse: 390.233\n",
      "[1000]\tvalid_0's rmse: 388.475\n",
      "[1100]\tvalid_0's rmse: 386.394\n",
      "[1200]\tvalid_0's rmse: 384.923\n",
      "[1300]\tvalid_0's rmse: 383.429\n",
      "[1400]\tvalid_0's rmse: 381.393\n",
      "[1500]\tvalid_0's rmse: 380.537\n",
      "[1600]\tvalid_0's rmse: 379.971\n",
      "[1700]\tvalid_0's rmse: 378.738\n",
      "[1800]\tvalid_0's rmse: 376.612\n",
      "[1900]\tvalid_0's rmse: 376.471\n",
      "[2000]\tvalid_0's rmse: 376.326\n",
      "[2100]\tvalid_0's rmse: 376.184\n",
      "[2200]\tvalid_0's rmse: 375.746\n",
      "[2300]\tvalid_0's rmse: 374.821\n",
      "[2400]\tvalid_0's rmse: 374.621\n",
      "[2500]\tvalid_0's rmse: 374.59\n",
      "[2600]\tvalid_0's rmse: 374.402\n",
      "[2700]\tvalid_0's rmse: 374.229\n",
      "Early stopping, best iteration is:\n",
      "[2547]\tvalid_0's rmse: 374.01\n"
     ]
    }
   ],
   "source": [
    "train_data = lgb.Dataset(X_train, label = Y_train, categorical_feature=category_cols)\n",
    "valid_data = lgb.Dataset(X_val, label = Y_val, categorical_feature=category_cols)\n",
    "\n",
    "lgbmodel = lgb.train(lgb_best_hyperparams, train_data, valid_sets=[valid_data],\n",
    "                     early_stopping_rounds=200, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fae09",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a203dc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict 2023-02-04\n",
      "Predict 2023-02-05\n",
      "Predict 2023-02-06\n",
      "Predict 2023-02-07\n",
      "Predict 2023-02-08\n",
      "Predict 2023-02-09\n",
      "Predict 2023-02-10\n",
      "Predict 2023-02-11\n",
      "Predict 2023-02-12\n",
      "Predict 2023-02-13\n",
      "Predict 2023-02-14\n",
      "Predict 2023-02-15\n",
      "Predict 2023-02-16\n",
      "Predict 2023-02-17\n",
      "Predict 2023-02-18\n",
      "Predict 2023-02-19\n",
      "Predict 2023-02-20\n",
      "Predict 2023-02-21\n",
      "Predict 2023-02-22\n",
      "Predict 2023-02-23\n",
      "Predict 2023-02-24\n",
      "Predict 2023-02-25\n",
      "Predict 2023-02-26\n",
      "Predict 2023-02-27\n",
      "Predict 2023-02-28\n",
      "Predict 2023-03-01\n",
      "Predict 2023-03-02\n",
      "Predict 2023-03-03\n"
     ]
    }
   ],
   "source": [
    "cols = [f\"F{i}\" for i in range(1,29)]\n",
    "valid_df[\"timestamp\"] = pd.to_datetime(valid_df[\"timestamp\"])\n",
    "for time_delta in range(0, 28):\n",
    "    day = datetime(2023, 2, 4) + timedelta(days=time_delta)\n",
    "    print(\"Predict\", day.date())\n",
    "    tst = valid_df[(valid_df.timestamp >= day - timedelta(days=60)) & (valid_df.timestamp <= day)].copy()\n",
    "    lags_wins(tst)\n",
    "    tst = tst.replace([np.inf, -np.inf, np.nan, -np.nan], 0)\n",
    "    tst = tst.loc[tst.timestamp == day , train_cols]\n",
    "    valid_df.loc[valid_df.timestamp == day, \"price\"] = lgbmodel.predict(tst)\n",
    "    del(tst)\n",
    "valid_pred_y_lgb = np.zeros((39, 1))\n",
    "for i in range(0, 28):\n",
    "    valid_pred_y_lgb = np.hstack([\n",
    "        valid_pred_y_lgb, valid_df.loc[valid_df.timestamp == (datetime(2023, 2, 4) + timedelta(days=i)), 'price'].values.reshape(-1, 1)])\n",
    "valid_pred_y_lgb = np.delete(valid_pred_y_lgb, 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b73af08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict 2023-03-04\n",
      "Predict 2023-03-05\n",
      "Predict 2023-03-06\n",
      "Predict 2023-03-07\n",
      "Predict 2023-03-08\n",
      "Predict 2023-03-09\n",
      "Predict 2023-03-10\n",
      "Predict 2023-03-11\n",
      "Predict 2023-03-12\n",
      "Predict 2023-03-13\n",
      "Predict 2023-03-14\n",
      "Predict 2023-03-15\n",
      "Predict 2023-03-16\n",
      "Predict 2023-03-17\n",
      "Predict 2023-03-18\n",
      "Predict 2023-03-19\n",
      "Predict 2023-03-20\n",
      "Predict 2023-03-21\n",
      "Predict 2023-03-22\n",
      "Predict 2023-03-23\n",
      "Predict 2023-03-24\n",
      "Predict 2023-03-25\n",
      "Predict 2023-03-26\n",
      "Predict 2023-03-27\n",
      "Predict 2023-03-28\n",
      "Predict 2023-03-29\n",
      "Predict 2023-03-30\n",
      "Predict 2023-03-31\n"
     ]
    }
   ],
   "source": [
    "cols = [f\"F{i}\" for i in range(1,29)]\n",
    "test_df[\"timestamp\"] = pd.to_datetime(test_df[\"timestamp\"])\n",
    "for time_delta in range(0, 28):\n",
    "    day = datetime(2023, 3, 4) + timedelta(days=time_delta)\n",
    "    print(\"Predict\", day.date())\n",
    "    tst = test_df[(test_df.timestamp >= day - timedelta(days=60)) & (test_df.timestamp <= day)].copy()\n",
    "    lags_wins(tst)\n",
    "    tst = tst.replace([np.inf, -np.inf, np.nan, -np.nan], 0)\n",
    "    tst = tst.loc[tst.timestamp == day , train_cols]\n",
    "    test_df.loc[test_df.timestamp == day, \"price\"] = lgbmodel.predict(tst)\n",
    "    del(tst)\n",
    "test_pred_y_lgb = np.zeros((39, 1))\n",
    "for i in range(0, 28):\n",
    "    test_pred_y_lgb = np.hstack([\n",
    "        test_pred_y_lgb, test_df.loc[test_df.timestamp == (datetime(2023, 2, 4) + timedelta(days=i)), 'price'].values.reshape(-1, 1)])\n",
    "test_pred_y_lgb = np.delete(test_pred_y_lgb, 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19742581",
   "metadata": {},
   "source": [
    "# 시계열 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ea152",
   "metadata": {},
   "source": [
    "## Simple Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da811401",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_df_pivot = valid_df.pivot(index=['item', 'corporation', 'location'], columns='d', values='price').reset_index()\n",
    "test_df_pivot = test_df.pivot(index=['item', 'corporation', 'location'], columns='d', values='price').reset_index()\n",
    "valid_df_pivot = valid_df_pivot.replace([np.inf, -np.inf, np.nan, -np.nan], 0)\n",
    "test_df_pivot = test_df_pivot.replace([np.inf, -np.inf, np.nan, -np.nan], 0)\n",
    "valid_df_pivot = valid_df_pivot[train_df.d.unique()]\n",
    "test_df_pivot = test_df_pivot[test_df.d.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e5295da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = valid_df_pivot.iloc[:, -28*4:-28]\n",
    "val_dataset = valid_df_pivot.iloc[:, -28:]\n",
    "train_data = test_df_pivot.iloc[:, -28-365:-28]\n",
    "val_data = test_df_pivot.iloc[:, -28:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32180ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 988.7071339446336\n"
     ]
    }
   ],
   "source": [
    "period = 30\n",
    "prediction_sma = train_dataset.iloc[:, -period:].copy() \n",
    "for i in range(len(val_dataset.loc[0])): \n",
    "    prediction_sma['F'+str(i+1)] = prediction_sma.iloc[:, -period:].mean(axis=1) \n",
    "prediction_sma = prediction_sma[['F'+str(i+1) for i in range(len(val_dataset.loc[0]))]] \n",
    "sma_rmse = mean_squared_error(prediction_sma, val_dataset)**0.5\n",
    "print('rmse:', sma_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2035f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 28\n",
    "test_pred_y_sma = train_data.iloc[:, -period:].copy() \n",
    "for i in range(len(val_data.loc[0])): \n",
    "    test_pred_y_sma['F'+str(i+1)] = test_pred_y_sma.iloc[:, -period:].mean(axis=1) \n",
    "test_pred_y_sma = test_pred_y_sma[['F'+str(i+1) for i in range(len(val_data.loc[0]))]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa2e2b",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75fb8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['d'].isin(days_train)==True]\n",
    "prophet_data = train.rename(columns={'timestamp': 'ds', 'price': 'y'})\n",
    "prophet_data = prophet_data[['ID', 'ds', 'y']]\n",
    "prophet_data['ID'] = prophet_data['ID'].str.replace(r'_\\d{8}$', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73c3ae9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:15:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:15:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:15:59 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "pred_list = []  \n",
    "for code in prophet_data['ID'].unique():\n",
    "    d = prophet_data[prophet_data['ID'] == code].reset_index().drop(['ID'], axis=1).sort_values('ds')\n",
    "    model = Prophet(\n",
    "        growth = 'linear',\n",
    "        seasonality_mode = 'additive',\n",
    "        yearly_seasonality = True,\n",
    "        weekly_seasonality = True,\n",
    "        daily_seasonality = True,\n",
    "#         holidays = True,\n",
    "#         changepoint_prior_scale = 0.1\n",
    "                   )\n",
    "    model.fit(d)\n",
    "    future = pd.DataFrame()\n",
    "    future['ds'] = pd.date_range(start='2023-02-04', periods=28, freq='D') \n",
    "    forecast = model.predict(future)   \n",
    "    pred_y = forecast['yhat'].values\n",
    "    pred_code = [str(code)] * len(pred_y)\n",
    "    for y_val, id_val in zip(pred_y, pred_code):\n",
    "        pred_list.append({'ID': id_val, 'y': y_val})\n",
    "pred = pd.DataFrame(pred_list)\n",
    "valid_pred_y_pro = np.where(pred['y']<0, 0, pred['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9c8e0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:15:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:15 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "pred_list = []  \n",
    "for code in prophet_data['ID'].unique():\n",
    "    d = prophet_data[prophet_data['ID'] == code].reset_index().drop(['ID'], axis=1).sort_values('ds')\n",
    "    model = Prophet(\n",
    "        growth = 'linear',\n",
    "        seasonality_mode = 'additive',\n",
    "        yearly_seasonality = True,\n",
    "        weekly_seasonality = True,\n",
    "        daily_seasonality = True,\n",
    "#         holidays = True,\n",
    "#         changepoint_prior_scale = 0.1\n",
    "                   )\n",
    "    model.fit(d)\n",
    "    future = pd.DataFrame()\n",
    "    future['ds'] = pd.date_range(start='2023-03-04', periods=28, freq='D') \n",
    "    forecast = model.predict(future)   \n",
    "    pred_y = forecast['yhat'].values\n",
    "    pred_code = [str(code)] * len(pred_y)\n",
    "    for y_val, id_val in zip(pred_y, pred_code):\n",
    "        pred_list.append({'ID': id_val, 'y': y_val})\n",
    "pred = pd.DataFrame(pred_list)\n",
    "test_pred_y_pro = np.where(pred['y']<0, 0, pred['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4ffdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266424b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class moving_avg(torch.nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = torch.nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "class series_decomp(torch.nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        residual = x - moving_mean\n",
    "        return moving_mean, residual \n",
    "        \n",
    "class LTSF_DLinear(torch.nn.Module):\n",
    "    def __init__(self, window_size, forcast_size, kernel_size, individual, feature_size):\n",
    "        super(LTSF_DLinear, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.forcast_size = forcast_size\n",
    "        self.decompsition = series_decomp(kernel_size)\n",
    "        self.individual = individual\n",
    "        self.channels = feature_size\n",
    "        if self.individual:\n",
    "            self.Linear_Seasonal = torch.nn.ModuleList()\n",
    "            self.Linear_Trend = torch.nn.ModuleList()\n",
    "            for i in range(self.channels):\n",
    "                self.Linear_Trend.append(torch.nn.Linear(self.window_size, self.forcast_size))\n",
    "                self.Linear_Trend[i].weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "                self.Linear_Seasonal.append(torch.nn.Linear(self.window_size, self.forcast_size))\n",
    "                self.Linear_Seasonal[i].weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "        else:\n",
    "            self.Linear_Trend = torch.nn.Linear(self.window_size, self.forcast_size)\n",
    "            self.Linear_Trend.weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "            self.Linear_Seasonal = torch.nn.Linear(self.window_size,  self.forcast_size)\n",
    "            self.Linear_Seasonal.weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        trend_init, seasonal_init = self.decompsition(x)\n",
    "        trend_init, seasonal_init = trend_init.permute(0,2,1), seasonal_init.permute(0,2,1)\n",
    "        if self.individual:\n",
    "            trend_output = torch.zeros([trend_init.size(0), trend_init.size(1), self.forcast_size], dtype=trend_init.dtype).to(trend_init.device)\n",
    "            seasonal_output = torch.zeros([seasonal_init.size(0), seasonal_init.size(1), self.forcast_size], dtype=seasonal_init.dtype).to(seasonal_init.device)\n",
    "            for idx in range(self.channels):\n",
    "                trend_output[:, idx, :] = self.Linear_Trend[idx](trend_init[:, idx, :])\n",
    "                seasonal_output[:, idx, :] = self.Linear_Seasonal[idx](seasonal_init[:, idx, :])                \n",
    "        else:\n",
    "            trend_output = self.Linear_Trend(trend_init)\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "        x = seasonal_output + trend_output\n",
    "        return x.permute(0,2,1)\n",
    "    \n",
    "class LTSF_NLinear(torch.nn.Module):\n",
    "    def __init__(self, window_size, forcast_size, individual, feature_size):\n",
    "        super(LTSF_NLinear, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.forcast_size = forcast_size\n",
    "        self.individual = individual\n",
    "        self.channels = feature_size\n",
    "        if self.individual:\n",
    "            self.Linear = torch.nn.ModuleList()\n",
    "            for i in range(self.channels):\n",
    "                self.Linear.append(torch.nn.Linear(self.window_size, self.forcast_size))\n",
    "        else:\n",
    "            self.Linear = torch.nn.Linear(self.window_size, self.forcast_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_last = x[:,-1:,:].detach()\n",
    "        x = x - seq_last\n",
    "        if self.individual:\n",
    "            output = torch.zeros([x.size(0), self.forcast_size, x.size(2)],dtype=x.dtype).to(x.device)\n",
    "            for i in range(self.channels):\n",
    "                output[:,:,i] = self.Linear[i](x[:,:,i])\n",
    "            x = output\n",
    "        else:\n",
    "            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)\n",
    "        x = x + seq_last\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d941bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(train_df, test_df, not_col, target):\n",
    "    train_df_ = train_df.copy()\n",
    "    test_df_ = test_df.copy()\n",
    "    col =  [col for col in list(train_df.columns) if col not in [not_col]]\n",
    "    mean_list = []\n",
    "    std_list = []\n",
    "    for x in col:\n",
    "        mean, std = train_df_.agg([\"mean\", \"std\"]).loc[:,x]\n",
    "        mean_list.append(mean)\n",
    "        std_list.append(std)\n",
    "        train_df_.loc[:, x] = (train_df_[x] - mean) / std\n",
    "        test_df_.loc[:, x] = (test_df_[x] - mean) / std\n",
    "    return train_df_, test_df_, mean_list[col.index(target)], std_list[col.index(target)]\n",
    "\n",
    "def time_slide_df(df, window_size, forcast_size, date, target):\n",
    "    df_ = df.copy()\n",
    "    data_list = []\n",
    "    dap_list = []\n",
    "    date_list = []\n",
    "    for idx in range(0, df_.shape[0]-window_size-forcast_size+1):\n",
    "        x = df_.loc[idx:idx+window_size-1, target].values.reshape(window_size, 1)\n",
    "        y = df_.loc[idx+window_size:idx+window_size+forcast_size-1, target].values\n",
    "        date_ = df_.loc[idx+window_size:idx+window_size+forcast_size-1, date].values\n",
    "        data_list.append(x)\n",
    "        dap_list.append(y)\n",
    "        date_list.append(date_)\n",
    "    return np.array(data_list, dtype='float32'), np.array(dap_list, dtype='float32'), np.array(date_list)\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "        \n",
    "### Univariable ###\n",
    "### 데이터 셋 생성 ###\n",
    "window_size = 72\n",
    "forcast_size= 24\n",
    "batch_size = 32\n",
    "targets = '전력사용량(kWh)'\n",
    "date = 'date_time'\n",
    "\n",
    "train_df_fe, test_df_fe, mean_, std_ = standardization(train_df, test_df, 'date_time', targets)\n",
    "train_x, train_y, train_date = time_slide_df(train_df_fe, window_size, forcast_size, date, targets)\n",
    "test_x, test_y, test_date = time_slide_df(test_df_fe, window_size, forcast_size, date, targets)\n",
    "\n",
    "train_ds = Data(train_x[:1000], train_y[:1000])\n",
    "valid_ds = Data(train_x[1000:], train_y[1000:])\n",
    "test_ds = Data(test_x, test_y)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle=True,)\n",
    "valid_dl = DataLoader(valid_ds, batch_size = train_x[1000:].shape[0], shuffle=False)\n",
    "test_dl  = DataLoader(test_ds,  batch_size = test_x.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "### 모델 학습 ###\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "test_loss_list = []\n",
    "epoch = 50\n",
    "lr = 0.001\n",
    "DLinear_model = LTSF_DLinear(\n",
    "                            window_size=window_size,\n",
    "                            forcast_size=forcast_size,\n",
    "                            kernel_size=25,\n",
    "                            individual=False,\n",
    "                            feature_size=1,\n",
    "                            )\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(DLinear_model.parameters(), lr=lr)\n",
    "max_loss = 999999999\n",
    "\n",
    "for epoch in tqdm(range(1, epoch+1)):\n",
    "    loss_list = []\n",
    "    DLinear_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        output = DLinear_model(data)\n",
    "        loss = criterion(output, target.unsqueeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())    \n",
    "    train_loss_list.append(np.mean(loss_list))\n",
    "\n",
    "    DLinear_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_dl:\n",
    "            output = DLinear_model(data)\n",
    "            valid_loss = criterion(output, target.unsqueeze(-1))\n",
    "            valid_loss_list.append(valid_loss)\n",
    "        \n",
    "        for data, target in test_dl:\n",
    "            output = DLinear_model(data)\n",
    "            test_loss = criterion(output, target.unsqueeze(-1))\n",
    "            test_loss_list.append(test_loss)\n",
    "\n",
    "    if valid_loss < max_loss:\n",
    "        torch.save(DLinear_model, 'DLinear_model.pth')\n",
    "        max_loss = valid_loss\n",
    "        print(\"valid_loss={:.3f}, test_los{:.3f}, Model Save\".format(valid_loss, test_loss))\n",
    "        dlinear_best_epoch = epoch\n",
    "        dlinear_best_train_loss = np.mean(loss_list)\n",
    "        dlinear_best_valid_loss = np.mean(valid_loss.item())\n",
    "        dlinear_best_test_loss = np.mean(test_loss.item())\n",
    "\n",
    "    print(\"epoch = {}, train_loss : {:.3f}, valid_loss : {:.3f}, test_loss : {:.3f}\".format(epoch, np.mean(loss_list), valid_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b251a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cdff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd350df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_arima = []\n",
    "for row in tqdm(train_dataset.values):\n",
    "    arima = sm.tsa.statespace.SARIMAX(row, order=(1,1,1), seasonal_order=(1, 1, 1, 12)).fit()\n",
    "    prediction_arima.append(arima.predict(0, 27, typ='levels'))\n",
    "# prediction_arima = np.where(np.array(prediction_arima)<0, 0, prediction_arima)\n",
    "sma_rmse = mean_squared_error(prediction_arima, val_dataset)**0.5\n",
    "print('rmse:', sma_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc71785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f77f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_y_sma = prediction_sma.values.copy().reshape(-1)\n",
    "valid_pred_y_lgb = valid_pred_y_lgb.reshape(-1)\n",
    "valid_pred_y_pro = valid_pred_y_pro.reshape(-1)\n",
    "val_dataset = val_dataset.values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2ebe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a191ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_y_sma = np.where(valid_pred_y_sma<100, 0, valid_pred_y_sma)\n",
    "valid_pred_y_lgb = np.where(valid_pred_y_lgb<100, 0, valid_pred_y_lgb)\n",
    "valid_pred_y_pro = np.where(valid_pred_y_pro<100, 0, valid_pred_y_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b428f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5207c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27b92659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302ce222f4194989bd9be70bd748b82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{(5, 1, 0): 962.7758764835003,\n",
       " (6, 1, 0): 962.8106687907305,\n",
       " (4, 1, 0): 964.8079755242035,\n",
       " (3, 1, 0): 972.3701421974372,\n",
       " (6, 2, 0): 972.3701421974372}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# candidate = np.arange(0, 1000)\n",
    "candidate = [0, 1, 2, 3, 4, 5, 6]\n",
    "permute = permutations(candidate, 3)\n",
    "score = {}\n",
    "for i in tqdm(list(permute)):\n",
    "    pred_permute = (\n",
    "                    valid_pred_y_sma * i[0] +\n",
    "                    valid_pred_y_lgb * i[1] +\n",
    "                    valid_pred_y_pro * i[2]\n",
    "                   )\n",
    "    score[i] = mean_squared_error(val_dataset, pred_permute/sum(i))**0.5\n",
    "\n",
    "score = dict(sorted(score.items(), key=lambda x: x[1], reverse=False)[:5])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "54a81ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "964.8079755242035"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rmse = mean_squared_error(\n",
    "    valid_pred_y_sma * 0.8 +\n",
    "    valid_pred_y_lgb * 0.2,\n",
    "    val_dataset) ** 0.5\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "532ebc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "988.7071339446336"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rmse = mean_squared_error(\n",
    "    valid_pred_y_sma,\n",
    "    val_dataset) ** 0.5\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16cb98a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1558.4144884769391"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rmse = mean_squared_error(\n",
    "    valid_pred_y_lgb,\n",
    "    val_dataset) ** 0.5\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "101b2a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1574.9352873349228"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rmse = mean_squared_error(\n",
    "    valid_pred_y_pro,\n",
    "    val_dataset) ** 0.5\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27c5cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_y_sma = test_pred_y_sma.reshape(-1)\n",
    "test_pred_y_lgb = test_pred_y_lgb.reshape(-1)\n",
    "test_pred_y_pro = test_pred_y_pro.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63725219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2058.60714286, 2050.09311224, 2123.3107234 , ..., 2809.38628044,\n",
       "       2758.47150474, 2715.55977277])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_y_sma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63693287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3984.74076715, 1434.12877025, 4582.75220801, ...,  403.58865146,\n",
       "        396.12781505,  390.02126666])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_y_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2c0637d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1864.,    0., 1837., ...,  574.,  523.,  529.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_y_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64336eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = (\n",
    "    test_pred_y_sma * 0 + \n",
    "    test_pred_y_lgb * 1 + \n",
    "    test_pred_y_pro * 0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf54d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['answer'] = pred_test\n",
    "submission_df.to_csv('3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5dbcf33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TG_A_J_20230304</td>\n",
       "      <td>1864.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TG_A_J_20230305</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TG_A_J_20230306</td>\n",
       "      <td>1837.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TG_A_J_20230307</td>\n",
       "      <td>1595.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TG_A_J_20230308</td>\n",
       "      <td>1747.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>RD_F_J_20230327</td>\n",
       "      <td>468.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>RD_F_J_20230328</td>\n",
       "      <td>531.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>RD_F_J_20230329</td>\n",
       "      <td>574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>RD_F_J_20230330</td>\n",
       "      <td>523.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>RD_F_J_20230331</td>\n",
       "      <td>529.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1092 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID  answer\n",
       "0     TG_A_J_20230304  1864.0\n",
       "1     TG_A_J_20230305     0.0\n",
       "2     TG_A_J_20230306  1837.0\n",
       "3     TG_A_J_20230307  1595.0\n",
       "4     TG_A_J_20230308  1747.0\n",
       "...               ...     ...\n",
       "1087  RD_F_J_20230327   468.0\n",
       "1088  RD_F_J_20230328   531.0\n",
       "1089  RD_F_J_20230329   574.0\n",
       "1090  RD_F_J_20230330   523.0\n",
       "1091  RD_F_J_20230331   529.0\n",
       "\n",
       "[1092 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1efe446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
